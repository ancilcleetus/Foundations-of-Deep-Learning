{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Layers in PyTorch"
      ],
      "metadata": {
        "id": "ejmAF3rR90aI"
      },
      "id": "ejmAF3rR90aI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b88962-f0bf-4aca-92fe-5b02f54f4f7a",
      "metadata": {
        "id": "30b88962-f0bf-4aca-92fe-5b02f54f4f7a"
      },
      "outputs": [],
      "source": [
        "# Basic Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input tensor x has a shape of (batch_size, input_features)\n",
        "    1. batch_size -> no of samples in input batch\n",
        "    2. input_features -> no of features in each sample\n",
        "- Weight matrix has a shape of (output_features, input_features)\n",
        "    1. output_features -> no of neurons in the layer\n",
        "    2. Each row of weight matrix corresponds to weights associated with one neuron in the layer\n",
        "    3. Each column of weight matrix corresponds to weights associated with one input feature\n",
        "- Bias has a shape of (output_features,)\n",
        "    1. One bias value for each neuron in the layer"
      ],
      "metadata": {
        "id": "comvLwyk2KNn"
      },
      "id": "comvLwyk2KNn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Layer"
      ],
      "metadata": {
        "id": "VBy_Sr_n1MO_"
      },
      "id": "VBy_Sr_n1MO_"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Q8pxYIL1GQFf"
      },
      "id": "Q8pxYIL1GQFf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Layer"
      ],
      "metadata": {
        "id": "MqDXog5m1btG"
      },
      "id": "MqDXog5m1btG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output Dimension = [(Input Dimension + 2 * Padding - Kernel Size) / Stride] + 1"
      ],
      "metadata": {
        "id": "8KiwuWfZ1v4Q"
      },
      "id": "8KiwuWfZ1v4Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Layers"
      ],
      "metadata": {
        "id": "fsWBKq9p1SZ3"
      },
      "id": "fsWBKq9p1SZ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Linear Layer"
      ],
      "metadata": {
        "id": "JMRky9Qu1ggN"
      },
      "id": "JMRky9Qu1ggN"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "u-26Z0t4GVTe"
      },
      "id": "u-26Z0t4GVTe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Conv2D Layer"
      ],
      "metadata": {
        "id": "r5LFZSpv1kXl"
      },
      "id": "r5LFZSpv1kXl"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomConv2D(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    super(CustomConv2D, self).__init__()\n",
        "\n",
        "    # Define Convolutional layer parameters\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "\n",
        "    # Initialize Convolutional weights and biases\n",
        "    self.weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "    self.bias = nn.Parameter(torch.randn(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Get the dimensions of the input tensor\n",
        "      batch_size, in_channels, height, width = x.size()\n",
        "\n",
        "      # Calculate dimensions after applying padding\n",
        "      padded_height = height + (padding * 2)\n",
        "      padded_width = width + (padding * 2)\n",
        "\n",
        "      # Calculate dimensions after applying stride\n",
        "      output_height = (padded_height - self.kernel_size) // self.stride + 1\n",
        "      output_width = (padded_width - self.kernel_size) // self.stride + 1\n",
        "\n",
        "      # Initialize output tensor\n",
        "      output = torch.zeros(batch_size, self.out_channels, output_height, output_width)\n",
        "\n",
        "      # Perform the convolution operation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DW83ZIAY1ndN"
      },
      "id": "DW83ZIAY1ndN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}