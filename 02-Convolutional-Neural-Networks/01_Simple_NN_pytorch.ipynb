{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple Neural Net with PyTorch"
      ],
      "metadata": {
        "id": "ejmAF3rR90aI"
      },
      "id": "ejmAF3rR90aI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Net Definition"
      ],
      "metadata": {
        "id": "mfrsCSZJR62A"
      },
      "id": "mfrsCSZJR62A"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "30b88962-f0bf-4aca-92fe-5b02f54f4f7a",
      "metadata": {
        "id": "30b88962-f0bf-4aca-92fe-5b02f54f4f7a"
      },
      "outputs": [],
      "source": [
        "# Basic Imports\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(32 * 32 * 3, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)\n",
        ")"
      ],
      "metadata": {
        "id": "OAZzkiIN1Sun"
      },
      "id": "OAZzkiIN1Sun",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleNN, self).__init__()\n",
        "    self.fc1 = nn.Linear(32 * 32 * 3, 128)  # First hidden layer with 128 neurons\n",
        "    self.fc2 = nn.Linear(128, 64)  # Second hidden layer with 64 neurons\n",
        "    self.fc3 = nn.Linear(64, 10)  # Output layer with 10 neurons (10 classes in CIFAR-10 dataset)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 32 * 32 * 3)  # Flatten the input image to a 1D tensor (-1 used as a placeholder to infer the batch size automatically)\n",
        "    x = torch.relu(self.fc1(x))  # Apply ReLU Actviation to the first hidden layer\n",
        "    x = torch.relu(self.fc2(x))  # Apply ReLU Activation to the second hidden layer\n",
        "    x = self.fc3(x)  # Output layer (no activation function)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "RnauycDW1kub"
      },
      "id": "RnauycDW1kub",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "fco6Dhh8R3y_"
      },
      "id": "fco6Dhh8R3y_"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "WCuz79Z8SKZM"
      },
      "id": "WCuz79Z8SKZM",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a series of image transformations\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        # Normalize pixel values in 3 channels (convert from [0, 1] to [-1, 1])\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Dataloaders that handle batching, shuffling & parallel data loading for train & test datasets\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiIV8q6ZSaim",
        "outputId": "7c8f757c-a1c0-45e7-bf32-2998de3d6e4c"
      },
      "id": "XiIV8q6ZSaim",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model, Loss function and Optimizer"
      ],
      "metadata": {
        "id": "Cb4XYjMXdXsv"
      },
      "id": "Cb4XYjMXdXsv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SimpleNN class\n",
        "model = SimpleNN()\n",
        "\n",
        "# Cross Entropy Loss for Multi-Class Classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Mini-Batch Stochastic Gradient Descent Optimizer\n",
        "# model.parameters() -> parameters to be updated\n",
        "# lr -> learning rate that controls stepsize of weight updates\n",
        "# momentum -> to accelerate convergence\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "Sc7a52qlcUUu"
      },
      "id": "Sc7a52qlcUUu",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SimpleNN model for n epochs"
      ],
      "metadata": {
        "id": "QGrQXbAIotBa"
      },
      "id": "QGrQXbAIotBa"
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data  # data is a list of [inputs, labels]\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward Pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Loss Calculation\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backpropagation (find gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Extract numerical value of loss\n",
        "    loss_value = loss.item()\n",
        "\n",
        "    # Print statistics\n",
        "    running_loss += loss_value\n",
        "\n",
        "    if (i + 1) % 2000 == 0:  # Print every 2000 mini-batches\n",
        "      print(f\"Epoch {epoch + 1} Mini-Batch {i + 1:5d} -> Loss = {running_loss / 2000:.3f}\")\n",
        "      running_loss = 0.0\n",
        "\n",
        "print(f\"Finished training for {n_epochs} epochs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVZ0vVwso01N",
        "outputId": "736e9caa-16d8-4428-9475-68a07db37cb1"
      },
      "id": "FVZ0vVwso01N",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Mini-Batch  2000 -> Loss = 1.945\n",
            "Epoch 1 Mini-Batch  4000 -> Loss = 1.732\n",
            "Epoch 1 Mini-Batch  6000 -> Loss = 1.678\n",
            "Epoch 1 Mini-Batch  8000 -> Loss = 1.649\n",
            "Epoch 1 Mini-Batch 10000 -> Loss = 1.578\n",
            "Epoch 1 Mini-Batch 12000 -> Loss = 1.566\n",
            "Epoch 2 Mini-Batch  2000 -> Loss = 1.499\n",
            "Epoch 2 Mini-Batch  4000 -> Loss = 1.482\n",
            "Epoch 2 Mini-Batch  6000 -> Loss = 1.500\n",
            "Epoch 2 Mini-Batch  8000 -> Loss = 1.464\n",
            "Epoch 2 Mini-Batch 10000 -> Loss = 1.447\n",
            "Epoch 2 Mini-Batch 12000 -> Loss = 1.465\n",
            "Epoch 3 Mini-Batch  2000 -> Loss = 1.388\n",
            "Epoch 3 Mini-Batch  4000 -> Loss = 1.366\n",
            "Epoch 3 Mini-Batch  6000 -> Loss = 1.385\n",
            "Epoch 3 Mini-Batch  8000 -> Loss = 1.371\n",
            "Epoch 3 Mini-Batch 10000 -> Loss = 1.375\n",
            "Epoch 3 Mini-Batch 12000 -> Loss = 1.387\n",
            "Epoch 4 Mini-Batch  2000 -> Loss = 1.306\n",
            "Epoch 4 Mini-Batch  4000 -> Loss = 1.302\n",
            "Epoch 4 Mini-Batch  6000 -> Loss = 1.316\n",
            "Epoch 4 Mini-Batch  8000 -> Loss = 1.295\n",
            "Epoch 4 Mini-Batch 10000 -> Loss = 1.332\n",
            "Epoch 4 Mini-Batch 12000 -> Loss = 1.321\n",
            "Epoch 5 Mini-Batch  2000 -> Loss = 1.244\n",
            "Epoch 5 Mini-Batch  4000 -> Loss = 1.245\n",
            "Epoch 5 Mini-Batch  6000 -> Loss = 1.262\n",
            "Epoch 5 Mini-Batch  8000 -> Loss = 1.269\n",
            "Epoch 5 Mini-Batch 10000 -> Loss = 1.277\n",
            "Epoch 5 Mini-Batch 12000 -> Loss = 1.263\n",
            "Epoch 6 Mini-Batch  2000 -> Loss = 1.183\n",
            "Epoch 6 Mini-Batch  4000 -> Loss = 1.216\n",
            "Epoch 6 Mini-Batch  6000 -> Loss = 1.220\n",
            "Epoch 6 Mini-Batch  8000 -> Loss = 1.220\n",
            "Epoch 6 Mini-Batch 10000 -> Loss = 1.224\n",
            "Epoch 6 Mini-Batch 12000 -> Loss = 1.254\n",
            "Epoch 7 Mini-Batch  2000 -> Loss = 1.151\n",
            "Epoch 7 Mini-Batch  4000 -> Loss = 1.168\n",
            "Epoch 7 Mini-Batch  6000 -> Loss = 1.173\n",
            "Epoch 7 Mini-Batch  8000 -> Loss = 1.182\n",
            "Epoch 7 Mini-Batch 10000 -> Loss = 1.200\n",
            "Epoch 7 Mini-Batch 12000 -> Loss = 1.200\n",
            "Epoch 8 Mini-Batch  2000 -> Loss = 1.105\n",
            "Epoch 8 Mini-Batch  4000 -> Loss = 1.118\n",
            "Epoch 8 Mini-Batch  6000 -> Loss = 1.138\n",
            "Epoch 8 Mini-Batch  8000 -> Loss = 1.147\n",
            "Epoch 8 Mini-Batch 10000 -> Loss = 1.163\n",
            "Epoch 8 Mini-Batch 12000 -> Loss = 1.199\n",
            "Epoch 9 Mini-Batch  2000 -> Loss = 1.078\n",
            "Epoch 9 Mini-Batch  4000 -> Loss = 1.107\n",
            "Epoch 9 Mini-Batch  6000 -> Loss = 1.097\n",
            "Epoch 9 Mini-Batch  8000 -> Loss = 1.108\n",
            "Epoch 9 Mini-Batch 10000 -> Loss = 1.131\n",
            "Epoch 9 Mini-Batch 12000 -> Loss = 1.139\n",
            "Epoch 10 Mini-Batch  2000 -> Loss = 1.022\n",
            "Epoch 10 Mini-Batch  4000 -> Loss = 1.058\n",
            "Epoch 10 Mini-Batch  6000 -> Loss = 1.088\n",
            "Epoch 10 Mini-Batch  8000 -> Loss = 1.099\n",
            "Epoch 10 Mini-Batch 10000 -> Loss = 1.109\n",
            "Epoch 10 Mini-Batch 12000 -> Loss = 1.111\n",
            "Finished training for 10 epochs\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}